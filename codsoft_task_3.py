# -*- coding: utf-8 -*-
"""codsoft_task 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fLGegD7mmYfvWkS7mdjvarv4obU7UbHP
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from sklearn.metrics import precision_score, recall_score, f1_score

# Load the dataset
dataset = pd.read_csv('/content/creditcard.csv')

# Split the dataset into features and target
X = dataset.drop('Class', axis=1)
y = dataset['Class']

# I've used the mean and standard deviation to Normalize the features by subtracting the mean and dividing by the standard deviation.
X = (X - X.mean()) / X.std()

# Spliting into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Handling class imbalance using SMOTE
smote = SMOTE()
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# I've used a list of models to train and evaluate them
models = [
    LogisticRegression(),
    RandomForestClassifier(n_estimators=100, random_state=42),
    DecisionTreeClassifier(random_state=42),
    AdaBoostClassifier(random_state=42),
    XGBClassifier(random_state=42)
]

# Training and evaluating models
for model in models:
    model_name = model.__class__.__name__
    model.fit(X_train_resampled, y_train_resampled)
    y_pred = model.predict(X_test)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    print(f"{model_name} Performance:")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1-score: {f1:.2f}")
    print()